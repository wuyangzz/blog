<!DOCTYPE html>
<html lang="zh-cn">
  <head>
  <meta http-equiv="content-type" content="text/html;charset=utf-8">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="robots" content="noodp"/>
  <meta name="author" content="wuyangzz">
  
  
  
  <link rel="prev" href="https://wuyangzz.github.io/2021/%E9%80%9A%E8%BF%87%E7%B1%BB%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%97%A0%E7%9B%91%E7%9D%A3%E5%85%89%E6%B5%81%E9%87%8F%E4%BC%B0%E8%AE%A1%E8%BD%AC%E6%8D%A2%E7%9A%84%E5%8F%AF%E9%9D%A0%E7%9B%91%E7%9D%A3/" />
  <link rel="next" href="https://wuyangzz.github.io/2021/bi-directional-semi-supervised-training-of-convolutional-neural-networks-for-ultrasound-elastography-displacement-estimation/" />
  <link rel="canonical" href="https://wuyangzz.github.io/2021/%E8%87%AA%E7%9B%91%E7%9D%A3%E6%96%87%E7%8C%AE%E7%BB%BC%E8%BF%B0/" />
  <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
  <link rel="manifest" href="/site.webmanifest">
  <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">
  <title>
       
       
           自监督文献综述 | wuyangzz
       
  </title>
  <meta name="title" content="自监督文献综述 | wuyangzz">
    
  
  <link rel="stylesheet" href="/font/iconfont.css">
  <link rel="stylesheet" href="/css/main.min.css">


  
  
 

<script type="application/ld+json">
 "@context" : "http://schema.org",
    "@type" : "BlogPosting",
    "mainEntityOfPage": {
         "@type": "WebPage",
         "@id": "https:\/\/wuyangzz.github.io\/"
    },
    "articleSection" : "posts",
    "name" : "自监督文献综述",
    "headline" : "自监督文献综述",
    "description" : "综合对比：    paper code Method KITTI 2012 KITTI 2015 Sintel Clean Sintel Final        train test train test(F1-all)   Jason J Y, Harley A W, Derpanis K G. Back to basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness[C]\/\/European Conference on Computer Vision. Springer, Cham, 2016: 3-10. https:\/\/github.com\/ryersonvisionlab\/unsupFlownet BackToBasic 11.3 9.9 – –   Ren Z, Yan J, Ni B, et al.",
    "inLanguage" : "zh-cn",
    "author" : "wuyangzz",
    "creator" : "wuyangzz",
    "publisher": "wuyangzz",
    "accountablePerson" : "wuyangzz",
    "copyrightHolder" : "wuyangzz",
    "copyrightYear" : "2021",
    "datePublished": "2021-07-28 15:09:37 \u002b0800 CST",
    "dateModified" : "2021-07-28 15:09:37 \u002b0800 CST",
    "url" : "https:\/\/wuyangzz.github.io\/2021\/%E8%87%AA%E7%9B%91%E7%9D%A3%E6%96%87%E7%8C%AE%E7%BB%BC%E8%BF%B0\/",
    "wordCount" : "588",
    "keywords" : [ "", "wuyangzz"]
}
</script>

</head>

  


  <body class="">
    <div class="wrapper">
        <nav class="navbar">
    <div class="container">
        <div class="navbar-header header-logo">
        	<a href="javascript:void(0);" class="theme-switch"><i class="iconfont icon-xihuan"></i></a>&nbsp;<a href="https://wuyangzz.github.io/">wuyangzz</a>
        </div>
        <div class="menu navbar-right">
                
                
                <a class="menu-item" href="/posts/" title="">博客</a>
                
                <a class="menu-item" href="/categories/" title="">分类</a>
                
                <a class="menu-item" href="/tags/" title="">标签</a>
                
                <a class="menu-item" href="/about/" title="关于我">关于我</a>
                
        </div>
    </div>
</nav>
<nav class="navbar-mobile" id="nav-mobile" style="display: none">
     <div class="container">
        <div class="navbar-header">
            <div>  <a href="javascript:void(0);" class="theme-switch"><i class="iconfont icon-xihuan"></i></a>&nbsp;<a href="https://wuyangzz.github.io/">wuyangzz</a></div>
            <div class="menu-toggle">
                <span></span><span></span><span></span>
            </div>
        </div>
     
          <div class="menu" id="mobile-menu">
                
                
                <a class="menu-item" href="/posts/" title="">博客</a>
                
                <a class="menu-item" href="/categories/" title="">分类</a>
                
                <a class="menu-item" href="/tags/" title="">标签</a>
                
                <a class="menu-item" href="/about/" title="关于我">关于我</a>
                
        </div>
    </div>
</nav>
    	 <main class="main">
          <div class="container">
      		
<article class="post-warp" itemscope itemtype="http://schema.org/Article">
    <header class="post-header">
        <h1 class="post-title" itemprop="name headline">自监督文献综述</h1>
        <div class="post-meta">
                Written by <a itemprop="name" href="https://wuyangzz.github.io/" rel="author">wuyangzz</a> with ♥ 
                <span class="post-time">
                on <time datetime=2021-07-28 itemprop="datePublished">July 28, 2021</time>
                </span>
                in
                <i class="iconfont icon-folder"></i>
                <span class="post-category">
                        <a href="https://wuyangzz.github.io/categories/">  </a>
                        
                </span>
        </div>
    </header>
    <div class="post-content">
        

        

        
        
     
          
          
          

          
          
          

          <h1 id="综合对比">综合对比：</h1>
<table>
<thead>
<tr>
<th>paper</th>
<th>code</th>
<th>Method</th>
<th>KITTI 2012</th>
<th>KITTI 2015</th>
<th>Sintel Clean</th>
<th>Sintel Final</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td></td>
<td>train</td>
<td>test</td>
<td>train</td>
<td>test(F1-all)</td>
</tr>
<tr>
<td>Jason J Y, Harley A W, Derpanis K G. Back to basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness[C]//European Conference on Computer Vision. Springer, Cham, 2016: 3-10.</td>
<td><a href="https://github.com/ryersonvisionlab/unsupFlownet">https://github.com/ryersonvisionlab/unsupFlownet</a></td>
<td>BackToBasic</td>
<td>11.3</td>
<td>9.9</td>
<td>–</td>
<td>–</td>
</tr>
<tr>
<td>Ren Z, Yan J, Ni B, et al. Unsupervised deep learning for optical flow estimation[C]//Thirty-First AAAI Conference on Artificial Intelligence. 2017.</td>
<td><a href="https://github.com/sunshinezhe/Dense-Spatial-Transform-Flow">https://github.com/sunshinezhe/Dense-Spatial-Transform-Flow</a></td>
<td>DSTFlow</td>
<td>10.43</td>
<td>12.4</td>
<td>16.79</td>
<td>39%</td>
</tr>
<tr>
<td>Meister S, Hur J, Roth S. Unflow: Unsupervised learning of optical flow with a bidirectional census loss[C]//Thirty-Second AAAI Conference on Artificial Intelligence. 2018.</td>
<td><a href="https://github.com/simonmeister/UnFlow">https://github.com/simonmeister/UnFlow</a></td>
<td>UnFlow</td>
<td>3.29</td>
<td>–</td>
<td>8.1</td>
<td>23.30%</td>
</tr>
<tr>
<td>Wang Y, Yang Y, Yang Z, et al. Occlusion aware unsupervised learning of optical flow[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 4884-4893.</td>
<td>–</td>
<td>OAFlow</td>
<td>3.55</td>
<td>4.2</td>
<td>8.88</td>
<td>31.20%</td>
</tr>
<tr>
<td>Janai J, Guney F, Ranjan A, et al. Unsupervised learning of multi-frame optical flow with occlusions[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 690-706.</td>
<td><a href="https://github.com/JJanai/back2future">https://github.com/JJanai/back2future</a></td>
<td>Back2Future</td>
<td>–</td>
<td>–</td>
<td>6.59</td>
<td>22.94%</td>
</tr>
<tr>
<td>Tian L, Tu Z, Zhang D, et al. Unsupervised learning of optical flow with cnn-based non-local filtering[J]. IEEE Transactions on Image Processing, 2020, 29: 8429-8442.</td>
<td>–</td>
<td>NLFlow</td>
<td>3.02</td>
<td>4.5</td>
<td>6.05</td>
<td>22.75%</td>
</tr>
<tr>
<td>Liu P, King I, Lyu M R, et al. Ddflow: Learning optical flow with unlabeled data distillation[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2019, 33(01): 8770-8777.</td>
<td><a href="https://github.com/ppliuboy/DDFlow">https://github.com/ppliuboy/DDFlow</a></td>
<td>DDFlow</td>
<td>2.35</td>
<td>3</td>
<td>5.72</td>
<td>14.29%</td>
</tr>
<tr>
<td>Zhong Y, Ji P, Wang J, et al. Unsupervised deep epipolar flow for stationary or dynamic scenes[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019: 12095-12104.</td>
<td><a href="https://github.com/yiranzhong/EPIflow">https://github.com/yiranzhong/EPIflow</a> （loss functions地址）</td>
<td>EpiFlow</td>
<td>2.51</td>
<td>3.4</td>
<td>5.55</td>
<td>16.95%</td>
</tr>
<tr>
<td>Liu P, Lyu M, King I, et al. Selflow: Self-supervised learning of optical flow[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019: 4571-4580.</td>
<td><a href="https://github.com/ppliuboy/SelFlow">https://github.com/ppliuboy/SelFlow</a></td>
<td>SelFlow</td>
<td>1.69</td>
<td>2.2</td>
<td>4.84</td>
<td>14.19%</td>
</tr>
<tr>
<td>Tian L, Tu Z, Zhang D, et al. Unsupervised learning of optical flow with cnn-based non-local filtering[J]. IEEE Transactions on Image Processing, 2020, 29: 8429-8442.</td>
<td>–</td>
<td>STFlow</td>
<td>1.64</td>
<td>1.9</td>
<td>3.56</td>
<td>13.83%</td>
</tr>
<tr>
<td>Liu L, Zhang J, He R, et al. Learning by analogy: Reliable supervision from transformations for unsupervised optical flow estimation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 6489-6498.</td>
<td><a href="https://github.com/lliuz/ARFlow">https://github.com/lliuz/ARFlow</a></td>
<td>ARFlow</td>
<td>1.44</td>
<td>1.8</td>
<td>2.85</td>
<td>11.80%</td>
</tr>
<tr>
<td>Im W, Kim T K, Yoon S E. Unsupervised learning of optical flow with deep feature similarity[C]//European Conference on Computer Vision. Springer, Cham, 2020: 172-188.</td>
<td><a href="https://github.com/iwbn/unsupsimflow">https://github.com/iwbn/unsupsimflow</a></td>
<td>SimFlow</td>
<td>–</td>
<td>–</td>
<td>5.19</td>
<td>13.38%</td>
</tr>
<tr>
<td>Jonschkowski R, Stone A, Barron J T, et al. What matters in unsupervised optical flow[C]//Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part II 16. Springer International Publishing, 2020: 557-572.</td>
<td><a href="https://github.com/google-research/google-research/tree/master/uflow">https://github.com/google-research/google-research/tree/master/uflow</a></td>
<td>UFlow</td>
<td>1.68</td>
<td>1.9</td>
<td>2.71</td>
<td>11.13%</td>
</tr>
<tr>
<td>Luo K, Wang C, Liu S, et al. Upflow: Upsampling pyramid for unsupervised optical flow learning[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 1045-1054.</td>
<td><a href="https://github.com/coolbeam/UPFlow">https://github.com/coolbeam/UPFlow</a>_pytorch</td>
<td>UPFlow</td>
<td>1.27</td>
<td>1.4</td>
<td>2.45</td>
<td>9.38%</td>
</tr>
<tr>
<td>Luo K, Luo A, Wang C, et al. ASFlow: Unsupervised Optical Flow Learning with Adaptive Pyramid Sampling[J]. arXiv preprint arXiv:2104.03560, 2021.</td>
<td>–</td>
<td>ASFlow</td>
<td>1.26</td>
<td>1.5</td>
<td>2.47</td>
<td>9.67%</td>
</tr>
<tr>
<td>Li J, Zhao J, Song S, et al. Occlusion aware unsupervised learning of optical flow from video[C]//Thirteenth International Conference on Machine Vision. International Society for Optics and Photonics, 2021, 11605: 116050T.</td>
<td><a href="https://github.com/CV-IP/UnOpticalFlow">https://github.com/CV-IP/UnOpticalFlow</a></td>
<td>UnOpticalFlow</td>
<td>2.67</td>
<td>7.1</td>
<td>22%</td>
<td>–</td>
</tr>
<tr>
<td>Luo K, Wang C, Ye N, et al. Occinpflow: Occlusion-inpainting optical flow estimation by unsupervised learning[J]. arXiv preprint arXiv:2006.16637, 2020.</td>
<td><a href="https://github.com/coolbeam/OccInpFlow#occinpflow-occlusion-inpainting-optical-flow-estimation-by-unsupervised-learning">https://github.com/coolbeam/OccInpFlow#occinpflow-occlusion-inpainting-optical-flow-estimation-by-unsupervised-learning</a></td>
<td>OccInpFlow</td>
<td>1.78</td>
<td>2.1</td>
<td>4.57</td>
<td>15.20%</td>
</tr>
</tbody>
</table>
<ul>
<li>
<p><strong>研究建议一</strong> ：可以与《Neural-network-based Motion Tracking for Breast Ultrasound Strain Elastography: An Initial Assessment of  erformance and Feasibility》文章类似思路。研究并比较上述自监督模型在超声B模式图像上在心动图上的一个数据情况。同时可以使用8个模拟心脏的模型和有限元和超声模拟数据进行验证。训练数据可以直接采用超声B模式的图像。</p>
</li>
<li>
<p>*<strong>研究建议二</strong>：如果上述具有可行性的话，因为里面如ARFlow模型借鉴了PWC-Net模型的金字塔网络的输入部分。那么就同理可以借鉴RFPWC-Net模型的输入部分。然后改损失函数。实现利用RF数据来作为训练模型的输入，原来使用RF数据训练模型最大的问题就是label不好确定。但是如果采用自监督模型的话就解决了最大的问题。然后两个结合起来应该是效果不错的，但是工作量比较大。后续可以挖掘的空间还很大。是一个很不错的点。</p>
</li>
</ul>

    </div>

    <div class="post-copyright">
             
            <p class="copyright-item">
                <span>Author:</span>
                <span>wuyangzz </span>
                </p>
            
           
             
            <p class="copyright-item">
                    <span>Link:</span>
                    <a href=https://wuyangzz.github.io/2021/%E8%87%AA%E7%9B%91%E7%9D%A3%E6%96%87%E7%8C%AE%E7%BB%BC%E8%BF%B0/>https://wuyangzz.github.io/2021/%E8%87%AA%E7%9B%91%E7%9D%A3%E6%96%87%E7%8C%AE%E7%BB%BC%E8%BF%B0/</span>
            </p>
            
            
    </div>

  
    <div class="post-tags">
        
            <section>
            <i class="iconfont icon-tag"></i>Tag(s): 
            
            <span class="tag"><a href="https://wuyangzz.github.io/tags//">
                    #</a></span>
            
            </section>
        
        <section>
                <a href="javascript:window.history.back();">back</a></span> · 
                <span><a href="https://wuyangzz.github.io/">home</a></span>
        </section>
    </div>

    <div class="post-nav">
        
        <a href="https://wuyangzz.github.io/2021/%E9%80%9A%E8%BF%87%E7%B1%BB%E6%AF%94%E5%AD%A6%E4%B9%A0%E6%97%A0%E7%9B%91%E7%9D%A3%E5%85%89%E6%B5%81%E9%87%8F%E4%BC%B0%E8%AE%A1%E8%BD%AC%E6%8D%A2%E7%9A%84%E5%8F%AF%E9%9D%A0%E7%9B%91%E7%9D%A3/" class="prev" rel="prev" title="通过类比学习：无监督光流量估计转换的可靠监督"><i class="iconfont icon-left"></i>&nbsp;通过类比学习：无监督光流量估计转换的可靠监督</a>
         
        
        <a href="https://wuyangzz.github.io/2021/bi-directional-semi-supervised-training-of-convolutional-neural-networks-for-ultrasound-elastography-displacement-estimation/" class="next" rel="next" title="Bi Directional Semi Supervised Training of Convolutional Neural Networks for Ultrasound Elastography Displacement Estimation">Bi Directional Semi Supervised Training of Convolutional Neural Networks for Ultrasound Elastography Displacement Estimation&nbsp;<i class="iconfont icon-right"></i></a>
        
    </div>

    <div class="post-comment">
          
                 
          
    </div>
</article>
          </div>
		   </main>
      <footer class="footer">
    <div class="copyright">
        &copy;
        
        <span itemprop="copyrightYear">2020 - 2022</span>
        
        <span class="with-love">
    	 <i class="iconfont icon-love"></i> 
         </span>
         
            <span class="author" itemprop="copyrightHolder"><a href="https://wuyangzz.github.io/">wuyangzz</a> | </span> 
         

         
	<span>Powered by <a href="https://github.com/wuyangzz/" target="_blank" rel="external nofollow">wuyangzz</a> & <a href="https://github.com/liuzc/leaveit" target="_blank" rel="external nofollow">LeaveIt</a></span>     </div>
</footer>
<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>











    
    
    <script src="/js/vendor_no_gallery.min.js" async=""></script>
    
  



     </div>
  </body>
</html>
