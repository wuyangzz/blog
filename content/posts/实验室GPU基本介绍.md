---
title: "实验室GPU基本介绍"
author: "wuyangzz"
tags: ["GPU"]
categories: ["实验室GPU"]
date: 2021-01-10T10:40:56+08:00
---

#	背景

​	实验室利用ESC 4000G2 服务器 32G内存 3T机械硬盘 4张3090显卡搭建GPU服务器。每一张3090峰值功耗为350w，服务器电源为1650w 在四张显卡不同时最高功率的情况下满足基本你的使用条件。但是因为3090非专业显卡。不支持vGPU功能。如果大家都直接操作宿主主机，直接在宿主主机上配置自己的开发环境的话肯定会发生冲突。所有最后经过实际考虑Docker进行合理的系统资源的搭配。使用 Docker 把服务器容器化，每个人都直接登录自己的容器，所有开发都在自己的容器内完成，这样就避免了冲突。并且，Docker 容器的额外开销小得可以忽略不计，所以也不会影响服务器性能。

# 解决方案

- 一个docker镜像就可以看作是一个操作系统。在docker上面进行的操作不会影响主机本生的环境
- 主机采用ubuntu或者centos作为宿主主机上的系统。
- 虚拟容器采用docker方式实现，为了能在docker中可以使用GPU。采用nvidia-docker进行gpu的加载。nvidia-docker 是专门为需要访问显卡资源的容器量身定制的，它对原始的 Docker 命令作了封装，只要使用 nvidia-docker run 命令运行容器，容器就可以访问主机显卡设备（只要主机安装了显卡驱动）。nvidia-docker 的使用规则和 Docker 是一致的，只需要把命令里的“docker”替换为“nvidia-docker”就可以了。
- 如果要在docker中使用显卡。现在NVIDIA给出的解决方案中必须使用linux系统。
- 可以在docker中加载基础的镜像，然后将22端口映射出来。就可以直接使用主机ip加映射的端口来访问和使用docker容器。
- 可以使用web界面如Shipyard等来对docker进行GUI管理
- NVIDIA有官方的Docker目录网站NGC，NGC为AI，机器学习和HPC提供了GPU加速容器的综合中心，这些容器已优化，测试并可以在本地和云中受支持的NVIDIA GPU上运行。此外，它提供了可以轻松集成到现有工作流程中的预训练模型，模型脚本和行业解决方案。
- NGC镜像中包含很多包，例如TensorFlow，PyTorch，MXNet，NVIDIA TensorRT™，RAPIDS等，并且有各个版本的组合可以下载。更新也非常快。

# 优势

- 用户可以方便地登录
- 用户可以自由安装软件
- 普通用户无法操作宿主主机
- 用户可以使用 GPU 资源
- 用户可以调动任意数量的GPU来共同计算。多人操作的时候也可以每个人指定一个GPU使用。
- 用户之间互不干扰